{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < Contents Type Predict Modeling >"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고 메시지 무시\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 학습 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>site</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>username</th>\n",
       "      <th>content</th>\n",
       "      <th>click</th>\n",
       "      <th>link</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>릴리바이레드 무스틴트</td>\n",
       "      <td>naverBlog</td>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>더블유랩 셀피립틴트! 부드러운 무스제형 벨벳틴트 장단점 확실하게 리뷰했어요</td>\n",
       "      <td>아영</td>\n",
       "      <td>안녕하세요! 뷰스타 아영입니다:)최근에 립제품이 정말 다양해지는 거 같더라구요!뻑뻑...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://m.blog.naver.com/loveneet?Redirect=Log...</td>\n",
       "      <td>바이럴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>릴리바이레드 무스틴트</td>\n",
       "      <td>naverBlog</td>\n",
       "      <td>2019-08-10</td>\n",
       "      <td>센카 클렌징티슈  후기</td>\n",
       "      <td>꽃사슴 소녀</td>\n",
       "      <td>안녕하세요!꽃사슴소녀입니다​​요즘같은 시국에일본제품을 리뷰한다는것이초콤 마음에 걸리...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://m.blog.naver.com/fairyasrai?Redirect=L...</td>\n",
       "      <td>바이럴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>릴리바이레드 무스틴트</td>\n",
       "      <td>naverBlog</td>\n",
       "      <td>2019-08-11</td>\n",
       "      <td>[Y.S.L] 입생로랑 틴트 후기 / 입생로랑 416 싸이키델릭칠리 / 입생로랑 바...</td>\n",
       "      <td>설연이네</td>\n",
       "      <td>#입생로랑#입생로랑416#입생로랑바이닐크림틴트생일선물로 받은 #입생로랑틴트백화점 틴...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://m.blog.naver.com/sy170416?Redirect=Log...</td>\n",
       "      <td>바이럴</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword       site       date  \\\n",
       "0  릴리바이레드 무스틴트  naverBlog 2019-08-09   \n",
       "1  릴리바이레드 무스틴트  naverBlog 2019-08-10   \n",
       "2  릴리바이레드 무스틴트  naverBlog 2019-08-11   \n",
       "\n",
       "                                               title username  \\\n",
       "0          더블유랩 셀피립틴트! 부드러운 무스제형 벨벳틴트 장단점 확실하게 리뷰했어요       아영   \n",
       "1                                       센카 클렌징티슈  후기   꽃사슴 소녀   \n",
       "2  [Y.S.L] 입생로랑 틴트 후기 / 입생로랑 416 싸이키델릭칠리 / 입생로랑 바...     설연이네   \n",
       "\n",
       "                                             content  click  \\\n",
       "0  안녕하세요! 뷰스타 아영입니다:)최근에 립제품이 정말 다양해지는 거 같더라구요!뻑뻑...    NaN   \n",
       "1  안녕하세요!꽃사슴소녀입니다​​요즘같은 시국에일본제품을 리뷰한다는것이초콤 마음에 걸리...    NaN   \n",
       "2  #입생로랑#입생로랑416#입생로랑바이닐크림틴트생일선물로 받은 #입생로랑틴트백화점 틴...    NaN   \n",
       "\n",
       "                                                link type  \n",
       "0  https://m.blog.naver.com/loveneet?Redirect=Log...  바이럴  \n",
       "1  https://m.blog.naver.com/fairyasrai?Redirect=L...  바이럴  \n",
       "2  https://m.blog.naver.com/sy170416?Redirect=Log...  바이럴  "
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드\n",
    "raw = pd.read_excel('data/학습데이터_v1.xlsx')\n",
    "\n",
    "# 데이터 확인\n",
    "raw[:3]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "소비자    98\n",
       "바이럴    71\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 선택\n",
    "data = raw[['content','type']]\n",
    "\n",
    "# 데이터 확인\n",
    "pd.value_counts(data['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텍스트 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "      <th>char_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>안녕하세요 뷰스타 아영입니다최근에 립제품이 정말 다양해지는 거 같더라구요뻑뻑한 립제...</td>\n",
       "      <td>바이럴</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>안녕하세요꽃사슴소녀입니다요즘같은 시국에일본제품을 리뷰한다는것이초콤 마음에 걸리지만이...</td>\n",
       "      <td>바이럴</td>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>입생로랑입생로랑입생로랑바이닐크림틴트생일선물로 받은 입생로랑틴트백화점 틴트를 잘 접하...</td>\n",
       "      <td>바이럴</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content type  char_len\n",
       "0  안녕하세요 뷰스타 아영입니다최근에 립제품이 정말 다양해지는 거 같더라구요뻑뻑한 립제...  바이럴      1032\n",
       "1  안녕하세요꽃사슴소녀입니다요즘같은 시국에일본제품을 리뷰한다는것이초콤 마음에 걸리지만이...  바이럴      1133\n",
       "2  입생로랑입생로랑입생로랑바이닐크림틴트생일선물로 받은 입생로랑틴트백화점 틴트를 잘 접하...  바이럴       757"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 텍스트 정제 함수\n",
    "def preprocessing_text(data):\n",
    "    url = \"(https?://)?(www.)?[a-zA-Z0-9./?=&-_]+[.]?[/][a-zA-Z0-9./?=&-_%]+\"\n",
    "    email = \"[-_.+a-zA-Z0-9]+[@].+[.][[a-zA-Z0-9]+|[a-zA-Z0-9]+[.][a-zA-Z0-9]+]\"\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = re.sub(url,\"\", data[i])\n",
    "        data[i] = re.sub(email,\"\", data[i])\n",
    "        data[i] = re.sub(\"[^가-힣a-zA-Z ]\",\"\", data[i])\n",
    "        data[i] = data[i].lower()\n",
    "    return data\n",
    "\n",
    "# 텍스트 정제\n",
    "data['content'] = preprocessing_text(data[\"content\"].astype('str'))\n",
    "data['char_len'] = [len(data['content'][_]) for _ in range(len(data['content']))]\n",
    "data = data.loc[data['char_len']>5].reset_index(drop=True)\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 형태소 분석 _Mecab"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [00:00<00:00, 201.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석기 Mecab\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "# mecab.morphs(clean_text[0])\n",
    "\n",
    "# 명사만 추출\n",
    "# mecab.nouns(data)\n",
    "\n",
    "# 품사 포함 추출\n",
    "# mecab.pos(clean_text[0],flatten=True)\n",
    "\n",
    "# 형태소 분석\n",
    "from tqdm import tqdm\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in tqdm(range(len(data['content']))):\n",
    "    sen = mecab.nouns(data['content'][i])\n",
    "    sen = [word for word in sen if len(word) > 1] #1글자 이상만 추출\n",
    "    corpus.append(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 단어 정제"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바꾸기\n",
    "clean_corpus = [[re.sub('^리바이$|^레드$','릴리바이레드',noun_) for noun_ in nouns] for nouns in corpus]\n",
    "\n",
    "# 제거\n",
    "clean_corpus = [[noun_ for noun_ in nouns if noun_ != \"영화\"] for nouns in clean_corpus]\n",
    "clean_corpus = [[noun_ for noun_ in nouns if noun_ != \"관람객\"] for nouns in clean_corpus]\n",
    "\n",
    "# 불용어 삭제\n",
    "f = open(\"data/stopwords_list.txt\", 'r', encoding='CP949')\n",
    "line = [line.rstrip() for line in f.readlines()]\n",
    "\n",
    "for i in range(len(line)):\n",
    "    clean_corpus = [[noun_ for noun_ in nouns if noun_ != line[i]] for nouns in clean_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 단어 빈도 수"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>릴리바이레드</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>틴트</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>컬러</td>\n",
       "      <td>941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>제품</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>사용</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>무스</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>벨벳</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>발색</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>느낌</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>추천</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  count\n",
       "0  릴리바이레드   1484\n",
       "1      틴트   1191\n",
       "2      컬러    941\n",
       "3      제품    727\n",
       "4      사용    397\n",
       "5      무스    394\n",
       "6      벨벳    345\n",
       "7      발색    340\n",
       "8      느낌    338\n",
       "9      추천    337"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 카운트 함수\n",
    "def count_noun(data):\n",
    "    t_noun = list()\n",
    "    \n",
    "    for _ in data:\n",
    "        t_noun.extend(_)\n",
    "    \n",
    "    t_noun_s = pd.DataFrame(t_noun, columns=['word'])\n",
    "    t_noun_f = t_noun_s.groupby('word').size().reset_index(name='count').sort_values(['count'], ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return t_noun_f\n",
    "\n",
    "# 단어 카운트\n",
    "frq_corpus = count_noun(clean_corpus)\n",
    "frq_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'뷰스타', '아영', '최근', '제품', '다양', '제품', '제품', '더블유', '피립', '틴트', '무스', '제형', '틴트', '소개', '더블유', '피립', '틴트', '가격', '머랭', '질감', '틴트', '더블유', '피립', '틴트', '가지', '색상', '가지', '색상', '사용', '가지', '색상', '사용', '릴리바이레드', '코랄', '핑크', '핑크', '다양', '핑크', '제품', '더블유', '셀피', '틴트', '플리', '이터', '보통', '당량', '플리', '이터', '호수', '버건', '릴리바이레드', '릴리바이레드', '코랄', '컬러', '소프트', '홀릭', '로즈', '핑크', '색상', '스윗', '퍼플', '핑크', '색상', '포커스', '취향', '더블유', '피립', '틴트', '릴리바이레드', '릴리바이레드', '컬러', '무스', '제형', '각질', '부각', '제품', '정도', '코랄', '색상', '소프트', '홀릭', '색상', '각질', '부각', '주름', '세상', '분위기', '출근', '지속력', '색상', '요채', '컬러', '스윗', '구애', '일리', '핑크', '컬러', '조심', '색상', '제품', '요호', '포커스', '컬러', '친구', '진단', '컬러', '색상', '친구', '입술', '입술', '바이', '입술', '사람', '바이', '사람', '평소', '입술', '각질', '부각', '벨벳', '추천', '사용', '입술', '더블유', '제품', '제공', '작성'\""
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(len(clean_corpus)):\n",
    "    clean_corpus[_] = re.sub(\"\\[|\\]\",\"\",str(clean_corpus[_]))\n",
    "clean_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_model = TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<169x3678 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 27606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model.fit(clean_corpus)\n",
    "tfidf_embedding = tfidf_model.transform(clean_corpus)\n",
    "tfidf_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델링\n",
    "### 1) 라벨 수치변환"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "      <th>char_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>안녕하세요 뷰스타 아영입니다최근에 립제품이 정말 다양해지는 거 같더라구요뻑뻑한 립제...</td>\n",
       "      <td>1</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>안녕하세요꽃사슴소녀입니다요즘같은 시국에일본제품을 리뷰한다는것이초콤 마음에 걸리지만이...</td>\n",
       "      <td>1</td>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>입생로랑입생로랑입생로랑바이닐크림틴트생일선물로 받은 입생로랑틴트백화점 틴트를 잘 접하...</td>\n",
       "      <td>1</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  type  char_len\n",
       "0  안녕하세요 뷰스타 아영입니다최근에 립제품이 정말 다양해지는 거 같더라구요뻑뻑한 립제...     1      1032\n",
       "1  안녕하세요꽃사슴소녀입니다요즘같은 시국에일본제품을 리뷰한다는것이초콤 마음에 걸리지만이...     1      1133\n",
       "2  입생로랑입생로랑입생로랑바이닐크림틴트생일선물로 받은 입생로랑틴트백화점 틴트를 잘 접하...     1       757"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_type = {\"바이럴\":1, \"소비자\":2}                                                                                                                               \n",
    "data['type'] = data['type'].map(c_type)\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 단어 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 416, 3012,  287, ...,    0,    0,    0],\n",
       "       [4408, 1060, 1389, ...,    0,    0,    0],\n",
       "       [ 166,  166,  166, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 520,  156,  184, ...,    0,    0,    0],\n",
       "       [7987, 7988, 7989, ...,    0,    0,    0],\n",
       "       [1053,   45,   60, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_corpus)\n",
    "sequences = tokenizer.texts_to_sequences(clean_corpus)\n",
    "\n",
    "word_vocab = tokenizer.word_index\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 424\n",
    "word_vector = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "word_vector"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 169, 169)"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_corpus), len(data), len(word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 라벨 균형 맞추기\n",
    "- Randomoversampler: 적은 수의 레이블 값을 복사  \n",
    "(실무에서는 오버피팅 문제로 잘 활용하지 않음)  \n",
    "  \n",
    "- Smote: 군집 알고리즘을 이용하여 비슷한 데이터 생성  \n",
    "(오버피팅 문제를 어느 정도 해결하여 실물에서 활용함)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=2020)\n",
    "oversampled_data, oversampled_label = ros.fit_resample(word_vector, label)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(k_neighbors=5, random_state=2020)\n",
    "smoted_data, smoted_label = smote.fit_resample(tfidf_embedding, data['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 머신러닝 알고리즘 학습"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics, model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<4x3678 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1121 stored elements in Compressed Sparse Row format>,\n",
       " array([2, 1]))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련데이터와 테스트 데이터 나누기\n",
    "train_data, test_data, train_label, test_label = train_test_split(smoted_data,smoted_label,random_state=0)\n",
    "train_data[:4], train_label[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBossting"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "param_grid = {\"n_estimators\":[1, 2, 3, 4, 5, 6,7,8,9,10], \n",
    "              \"learning_rate\":[0.001, 0.01, 0.1, 1, 10, 100],\n",
    "             \"max_depth\":[1,2,3,4,5]}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률: 0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "Gr_clf = GradientBoostingClassifier()\n",
    "Gr_clf.fit(train_data, train_label) #학습\n",
    "predict = Gr_clf.predict(test_data) #예측\n",
    "score = metrics.accuracy_score(test_label, predict)\n",
    "print(\"정답률:\", score)\n",
    "# scores = model_selection.cross_val_score(GridSearchCV(Gr_clf, param_grid, cv=5), test_data, test_label, cv=10)\n",
    "# print(\"교차검증 정답률 평균:\", scores.mean())\n",
    "# report = metrics.classification_report(test_label, predict)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답률: 0.9795918367346939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.96      0.98        24\n",
      "           2       0.96      1.00      0.98        25\n",
      "\n",
      "    accuracy                           0.98        49\n",
      "   macro avg       0.98      0.98      0.98        49\n",
      "weighted avg       0.98      0.98      0.98        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GB_grid = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)\n",
    "GB_grid.fit(train_data, train_label) #학습\n",
    "predict = GB_grid.predict(test_data) #예측\n",
    "score = metrics.accuracy_score(test_label, predict)\n",
    "print(\"정답률:\", score)\n",
    "report = metrics.classification_report(test_label, predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type_modeling.pkl']"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(GB_grid, 'type_modeling.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
